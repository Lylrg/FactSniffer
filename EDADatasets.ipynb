{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('Datasets/real-fake-news/Fake.csv')\n",
    "df_true = pd.read_csv('Datasets/real-fake-news/True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23481 entries, 0 to 23480\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    23481 non-null  object\n",
      " 1   text     23481 non-null  object\n",
      " 2   subject  23481 non-null  object\n",
      " 3   date     23481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 733.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_fake.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.drop([\"subject\",\"date\",\"title\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_polarity(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake['polarity'] = df_fake['text'].apply(calculate_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.005004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.023118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.011722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  label  polarity  \n",
       "0  December 31, 2017      0  0.082132  \n",
       "1  December 31, 2017      0 -0.005004  \n",
       "2  December 30, 2017      0 -0.012345  \n",
       "3  December 29, 2017      0 -0.023118  \n",
       "4  December 25, 2017      0 -0.011722  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21417 entries, 0 to 21416\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    21417 non-null  object\n",
      " 1   text     21417 non-null  object\n",
      " 2   subject  21417 non-null  object\n",
      " 3   date     21417 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 669.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_true.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politicsNews    11272\n",
       "worldnews       10145\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News               9050\n",
       "politics           6841\n",
       "left-news          4459\n",
       "Government News    1570\n",
       "US_News             783\n",
       "Middle-east         778\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true.drop([\"subject\",\"date\",\"title\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COVID ddata collected by Susan Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df_covid = pd.read_csv('Datasets/covid_fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to the recent outbreak for the Coronavirus...</td>\n",
       "      <td>You just need to add water, and the drugs and ...</td>\n",
       "      <td>coronavirusmedicalkit.com</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydroxychloroquine has been shown to have a 10...</td>\n",
       "      <td>RudyGiuliani</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fact: Hydroxychloroquine has been shown to hav...</td>\n",
       "      <td>CharlieKirk</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Corona virus is a man made virus created i...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doesn’t @BillGates finance research at the Wuh...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Due to the recent outbreak for the Coronavirus...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  You just need to add water, and the drugs and ...   \n",
       "1  Hydroxychloroquine has been shown to have a 10...   \n",
       "2  Fact: Hydroxychloroquine has been shown to hav...   \n",
       "3  The Corona virus is a man made virus created i...   \n",
       "4  Doesn’t @BillGates finance research at the Wuh...   \n",
       "\n",
       "                      source label  \n",
       "0  coronavirusmedicalkit.com  Fake  \n",
       "1               RudyGiuliani  Fake  \n",
       "2                CharlieKirk  Fake  \n",
       "3    JoanneWrightForCongress  Fake  \n",
       "4    JoanneWrightForCongress  Fake  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRUE    584\n",
       "Fake    345\n",
       "fake    230\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map sources to labels\n",
    "source_to_label = {\n",
    "    'TRUE': 1,\n",
    "    'Fake': 0,\n",
    "    'fake': 0\n",
    "}\n",
    "\n",
    "# Add 'label' column based on label'\n",
    "df_covid['label'] = df_covid['label'].apply(lambda x: source_to_label.get(x, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>Could the Power of the Sun Slow the Coronavirus?</td>\n",
       "      <td>A study suggests that ultraviolet rays could s...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Key evidence for coronavirus spread is flawed ...</td>\n",
       "      <td>Last week, a medical journal reported that a b...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>Summer Heat May Not Diminish Coronavirus Strength</td>\n",
       "      <td>A new report, sent to the White House science ...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>How Long Will a Vaccine Really Take?</td>\n",
       "      <td>A vaccine would be the ultimate weapon against...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>Why Funding the Covid-19 Response Could Be the...</td>\n",
       "      <td>Developing and delivering coronavirus vaccines...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "1159   Could the Power of the Sun Slow the Coronavirus?   \n",
       "1160  Key evidence for coronavirus spread is flawed ...   \n",
       "1161  Summer Heat May Not Diminish Coronavirus Strength   \n",
       "1162               How Long Will a Vaccine Really Take?   \n",
       "1163  Why Funding the Covid-19 Response Could Be the...   \n",
       "\n",
       "                                                   text  \\\n",
       "1159  A study suggests that ultraviolet rays could s...   \n",
       "1160  Last week, a medical journal reported that a b...   \n",
       "1161  A new report, sent to the White House science ...   \n",
       "1162  A vaccine would be the ultimate weapon against...   \n",
       "1163  Developing and delivering coronavirus vaccines...   \n",
       "\n",
       "                        source  label  \n",
       "1159  https://www.nytimes.com/      1  \n",
       "1160  https://www.nytimes.com/      1  \n",
       "1161  https://www.nytimes.com/      1  \n",
       "1162  https://www.nytimes.com/      1  \n",
       "1163  https://www.nytimes.com/      1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.drop([\"source\",\"title\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli = pd.read_csv('politifact.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli.drop([\"Unnamed: 0\",\"author\",\"source\", \"date\", \"target\", \"BinaryTarget\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>BinaryNumTarget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Video muestra a Rita Moreno hablando con Jorge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Congresswoman (Anna Paulina) Luna confirms th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Video shows a banner with Iran’s supreme leade...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Photo shows a pink dolphin spotted off the Nor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Caitlin Clark rejects $400 million deal from ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  BinaryNumTarget\n",
       "0  Video muestra a Rita Moreno hablando con Jorge...                0\n",
       "1  “Congresswoman (Anna Paulina) Luna confirms th...                0\n",
       "2  Video shows a banner with Iran’s supreme leade...                0\n",
       "3  Photo shows a pink dolphin spotted off the Nor...                0\n",
       "4  “Caitlin Clark rejects $400 million deal from ...                0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poli.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['statement', 'BinaryNumTarget'], dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poli.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "poli = poli.rename(columns={'statement':'text', 'BinaryNumTarget':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame()\n",
    "dataset=pd.concat([df_true,df_fake, df_covid, poli],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reseting index of dataset\n",
    "dataset.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/lydia/opt/anaconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp39-cp39-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.1 MB 1.4 MB/s eta 0:00:01    |█████████████████████████▋      | 9.7 MB 1.5 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/lydia/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/lydia/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 2.2.0\n",
      "    Uninstalling threadpoolctl-2.2.0:\n",
      "      Successfully uninstalled threadpoolctl-2.2.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lydia/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator CountVectorizer from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Load the fitted CountVectorizer\n",
    "loaded_cv = joblib.load('count_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loaded_rf \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrf.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 587\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 506\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[1;32m    508\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[1;32m    512\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "loaded_rf = joblib.load('rf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      "*> OR bruja\n",
      "\n",
      "Ahora entiendo, desde la plandemia se\n",
      "celebran mucho más los cumpleaños, está\n",
      "todo unido. Personas que conozco que\n",
      "odiaban los cumpleaños propios y ajenos,\n",
      "ahora lo celebran por todo lo alto.\n",
      "\n",
      "14:08 - 18 jun. 24 - 56 Visualizaciones\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [94]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(extracted_text)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Step 5: Load your pretrained machine learning model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Load the trained RandomForest model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m loaded_rf \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrf.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Load the fitted CountVectorizer\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loaded_cv \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_vectorizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 587\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 506\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[1;32m    508\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[1;32m    512\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import joblib\n",
    "\n",
    "# Step 2: Determine the path to the Tesseract executable\n",
    "# (You need to do this in the terminal and replace the path below with the actual path)\n",
    "# Command to run in the terminal:\n",
    "# which tesseract\n",
    "\n",
    "# Set the path to the Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/local/bin/tesseract'  # Replace with the actual path\n",
    "\n",
    "# Step 3: Load an image\n",
    "image_path = 'Captura de pantalla 2024-06-25 a las 22.56.43.png'  # Replace with your image path\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Step 4: Extract text from the image using the Spanish language model\n",
    "extracted_text = pytesseract.image_to_string(image, lang='spa')\n",
    "\n",
    "print(\"Extracted Text:\")\n",
    "print(extracted_text)\n",
    "\n",
    "# Step 5: Load your pretrained machine learning model\n",
    "# Load the trained RandomForest model\n",
    "loaded_rf = joblib.load('rf.pkl')\n",
    "\n",
    "# Load the fitted CountVectorizer\n",
    "loaded_cv = joblib.load('count_vectorizer.pkl')\n",
    "\n",
    "# Step 6: Make predictions using extracted text\n",
    "# Transform the extracted text using the loaded CountVectorizer\n",
    "extracted_text_transformed = loaded_cv.transform([extracted_text])\n",
    "\n",
    "# Make predictions using the loaded RandomForest model\n",
    "prediction = loaded_rf.predict(extracted_text_transformed)\n",
    "\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any NaN values\n",
    "dataset = dataset.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=14.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=14.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=15.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=15.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=12.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=23.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=24.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=24.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=18.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=14.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=57.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=59.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=15.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=69.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 7.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 7.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=69.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 6.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 6.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=11.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=38.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 9.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 9.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 8.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 8.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=17.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=17.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 3.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=17.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=17.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 3.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 3.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 3.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 2.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 5.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=13.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 5.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 5.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 5.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 5.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=10.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=10.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=10.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=10.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 4.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 3.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 3.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 3.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=14.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 7.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 7.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 7.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 7.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 7.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=15.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=15.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=16.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 4.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=16.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 4.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 3.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 3.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 3.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=17.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 7.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 7.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 7.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 7.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 7.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=14.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=14.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=14.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 3.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=14.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 3.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 3.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 3.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 3.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=13.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 6.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 6.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 6.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 6.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 6.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=13.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=13.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=13.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=13.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 2.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 4.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=12.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 4.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 4.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 4.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 4.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 9.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 9.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 9.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 9.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 9.0min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 4.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 4.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 4.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 4.7min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 4.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 9.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 9.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 9.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 9.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 2.4min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 2.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 8.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 3.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 3.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 3.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 4.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 3.6min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 4.9min\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 4.9min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  34.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  33.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  34.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 5.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  19.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  16.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 5.3min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  54.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  57.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  32.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  32.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 8.2min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.2min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  35.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  34.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  32.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.2min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  38.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  56.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  53.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  52.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  16.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  16.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  54.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  20.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  29.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  34.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 1.2min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  58.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  37.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  30.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  32.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 1.9min\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 2.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  34.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  34.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  33.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  33.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  33.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  34.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  34.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  34.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  33.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 2.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  58.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  46.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  36.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 2.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  32.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  33.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  32.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  33.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  33.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  34.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  33.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  32.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  33.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  32.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 1.9min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  43.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  39.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  32.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  45.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  48.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  25.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  34.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  33.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  33.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  32.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 1.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 2.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 2.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 2.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  32.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 2.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  32.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  33.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  33.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  33.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 2.1min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 1.0min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  56.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  41.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  31.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  53.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  57.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time= 1.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time= 1.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 5.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 5.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 5.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 5.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 1.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 4.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 4.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 4.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  32.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 1.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 3.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 2.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 3.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 2.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 2.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 4.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 4.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 4.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 4.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 4.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 4.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 4.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 4.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 4.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 4.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 4.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 4.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 4.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 4.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 2.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 2.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 1.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 3.7min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 4.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 4.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  34.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 3.6min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 1.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 1.5min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 2.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 2.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 2.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 4.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 4.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 4.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 4.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 1.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 1.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 4.2min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 2.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 2.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 2.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 2.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 2.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 4.1min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time= 1.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time= 2.0min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time= 2.0min\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 4.1min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time= 2.1min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time= 2.0min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 3.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 3.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 7.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 7.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 7.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 1.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 1.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 7.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 1.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 1.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time= 1.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 3.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 7.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 3.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 3.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 3.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 3.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 6.1min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 6.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 1.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 6.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 6.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 1.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 1.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 1.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time= 1.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 7.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 6.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 6.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 6.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 6.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time= 1.6min\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 6.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 3.2min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 3.2min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 3.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 7.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 7.6min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 7.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 1.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 7.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 1.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 1.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 1.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time= 1.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 5.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 2.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 5.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 5.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 1.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 5.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 5.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 1.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 1.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 1.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time= 1.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 5.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 2.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 2.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 2.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 2.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 2.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 5.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 5.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 5.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 5.6min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 1.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time= 1.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 5.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 2.6min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 2.6min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 2.6min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 2.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 4.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 4.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 4.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 4.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 1.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 1.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 1.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 4.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 4.6min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 4.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 4.7min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 1.2min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 4.8min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 1.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 1.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 1.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time= 1.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 5.1min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 2.4min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 2.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 4.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 4.5min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 4.3min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 3.9min\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 2.4min\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=23.1min\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=23.5min\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=23.9min\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=24.1min\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=47.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set the MLflow tracking URI to the correct port\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "# Test train data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.text, dataset.label, test_size=0.2, random_state=1)\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "cv_1 = cv.fit_transform(X_train)\n",
    "\n",
    "# Define the parameter grid for RandomForest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define the parameter grid for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize classifiers\n",
    "rf = RandomForestClassifier(random_state=1)\n",
    "svm = SVC(random_state=1)\n",
    "\n",
    "# Perform GridSearchCV with cross-validation for RandomForest\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(cv_1, y_train)\n",
    "\n",
    "# Perform GridSearchCV with cross-validation for SVM\n",
    "grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_svm.fit(cv_1, y_train)\n",
    "\n",
    "# Get the best models from grid search\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "\n",
    "# Transform the test data\n",
    "cv_test = cv.transform(X_test)\n",
    "\n",
    "# Predict on the test data using both models\n",
    "y_pred_rf = best_rf.predict(cv_test)\n",
    "y_pred_svm = best_svm.predict(cv_test)\n",
    "\n",
    "# Classification reports\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "report_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "print(\"Random Forest Classifier Report:\\n\", report_rf)\n",
    "print(\"SVM Classifier Report:\\n\", report_svm)\n",
    "\n",
    "# Log parameters and metrics with MLflow for RandomForest\n",
    "with mlflow.start_run(run_name=\"RandomForest\"):\n",
    "    # Log the parameters\n",
    "    mlflow.log_params(grid_search_rf.best_params_)\n",
    "\n",
    "    # Log the metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred_rf))\n",
    "    mlflow.log_metric(\"precision\", precision_score(y_test, y_pred_rf, average='weighted'))\n",
    "    mlflow.log_metric(\"recall\", recall_score(y_test, y_pred_rf, average='weighted'))\n",
    "    mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred_rf, average='weighted'))\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(best_rf, \"model\")\n",
    "\n",
    "    # Log the classification report as an artifact\n",
    "    with open(\"classification_report_rf.txt\", \"w\") as f:\n",
    "        f.write(report_rf)\n",
    "    mlflow.log_artifact(\"classification_report_rf.txt\")\n",
    "\n",
    "# Log parameters and metrics with MLflow for SVM\n",
    "with mlflow.start_run(run_name=\"SVM\"):\n",
    "    # Log the parameters\n",
    "    mlflow.log_params(grid_search_svm.best_params_)\n",
    "\n",
    "    # Log the metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred_svm))\n",
    "    mlflow.log_metric(\"precision\", precision_score(y_test, y_pred_svm, average='weighted'))\n",
    "    mlflow.log_metric(\"recall\", recall_score(y_test, y_pred_svm, average='weighted'))\n",
    "    mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred_svm, average='weighted'))\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(best_svm, \"model\")\n",
    "\n",
    "    # Log the classification report as an artifact\n",
    "    with open(\"classification_report_svm.txt\", \"w\") as f:\n",
    "        f.write(report_svm)\n",
    "    mlflow.log_artifact(\"classification_report_svm.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
